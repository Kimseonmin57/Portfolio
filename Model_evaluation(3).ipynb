{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "NuPxb9tabEv6",
    "outputId": "23c195da-c790-43aa-be12-a6cfe8080ff7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import glob\n",
    "from keras.models import Sequential\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aN_G3ECGAk5"
   },
   "outputs": [],
   "source": [
    "#### 이 셀은 밑에 아래 validation_matrix셀 실행 시, input ~~ cannot retrieve이런 식의 에러가 뜨면 실행해주세요. 그게 드라이브에서 불러오면 뭐가 불안해서 나는 에러라고합니다.\n",
    "import tarfile\n",
    "fname = '/content/drive/My Drive/Data_Augmentation.tar'  # 압축 파일을 지정해주고   \n",
    "ap = tarfile.open(fname)      # 열어줍니다. \n",
    "\n",
    "ap.extractall('/content/Data')         # 그리고는 압축을 풀어줍니다. \n",
    "# () 안에는 풀고 싶은 경로를 넣어주면 되요. 비워둘 경우 현재 경로에 압축 풉니다. \n",
    "\n",
    "ap.close()  \n",
    "\n",
    "fname = '/content/drive/My Drive/augmented_validation.tar'  # 압축 파일을 지정해주고   \n",
    "ap = tarfile.open(fname)      # 열어줍니다. \n",
    "\n",
    "ap.extractall('/content/Data')         # 그리고는 압축을 풀어줍니다. \n",
    "# () 안에는 풀고 싶은 경로를 넣어주면 되요. 비워둘 경우 현재 경로에 압축 풉니다. \n",
    "\n",
    "ap.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "76jNhooGM2Rv",
    "outputId": "5c9dabd3-33b3-47b8-e028-a72fe6d1c295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, None, None, 2048)  23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (None, 91)                547931    \n",
      "=================================================================\n",
      "Total params: 24,135,643\n",
      "Trainable params: 5,013,595\n",
      "Non-trainable params: 19,122,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model = load_model(model_path)\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7c6MfgE_NIYP"
   },
   "outputs": [],
   "source": [
    "from keras import applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vygjFc0DVbfU"
   },
   "source": [
    "여기서 부터 보면 됌!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "model_path = \"/content/Data/fine_tuned_model_totalV2.h5\"\n",
    "validation_data_dir = \"/content/Data/augmented_validation\"  # 0,1 과 같은 카테고리 번호 전까지\n",
    "\n",
    "#### 버전 1 : ResNet50통과 후 GlobalAveragePooling2D까지만 한 벡터 활용\n",
    "def making_validation_matrix(validation_data_dir, model_path):\n",
    "    ## making vectorizing model\n",
    "    vectorizing_model = Sequential()\n",
    "    model = load_model(model_path)\n",
    "    for layer in model.layers[:2]:\n",
    "        vectorizing_model.add(layer)\n",
    "    \n",
    "    ## making validation matrix\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    validation_matrix = vectorizing_model.predict_generator(generator)\n",
    "    \n",
    "    return vectorizing_model, validation_matrix\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    vectorizing_model, validation_matrix = making_validation_matrix(validation_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "Sti9LEdlLfMh",
    "outputId": "14a77ef9-b026-4fb3-c6c4-ff6aed2368c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10101, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_f8_YCLHAZc"
   },
   "outputs": [],
   "source": [
    "#vectorizing_model.save(\"/content/drive/My Drive/vectorizing_model.h5\")\n",
    "np.save(\"/content/drive/Shared drives/선빵팀 :) 2/validation_matrix/resnet50_matrix.npy\",validation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8p2yHUAIEcs8"
   },
   "outputs": [],
   "source": [
    "def original(img_path):\n",
    "    img = load_img(img_path)\n",
    "    img = img_to_array(img)\n",
    "    bgr_image = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "    return bgr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J3C7CEMhHy1q"
   },
   "outputs": [],
   "source": [
    "# adaptive threshold만 적용\n",
    "def threshold(imgpath):\n",
    "    # grayscale 변환\n",
    "    img = load_img(imgpath)\n",
    "    img = img_to_array(img)\n",
    "    gray_image = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #(a,b) -> 2차원으로 변환\n",
    "    gray_image = gray_image.astype('uint8')\n",
    "    \n",
    "    # adaptive threshold 적용\n",
    "    th = cv2.adaptiveThreshold(gray_image,\n",
    "                               255,\n",
    "                               cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                               cv2.THRESH_BINARY,\n",
    "                               99,\n",
    "                               15)\n",
    "    \n",
    "    # bgr 변환\n",
    "    bgr_image = cv2.cvtColor(th,cv2.COLOR_GRAY2BGR)\n",
    "    return bgr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0Z2iD4pH1rB"
   },
   "outputs": [],
   "source": [
    "# 이미지 대비 향상 -> adaptive threshold\n",
    "def threshold_sole(imgpath):\n",
    "    # grayscale 변환\n",
    "    img = load_img(imgpath)\n",
    "    img = img_to_array(img)\n",
    "    gray_image = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #(a,b) -> 2차원으로 변환\n",
    "    gray_image = gray_image.astype('uint8')\n",
    "    \n",
    "    # 이미지 대비를 향상\n",
    "    image_enhanced = cv2.equalizeHist(gray_image)\n",
    "    \n",
    "    # Adaptive Thresholding 적용 \n",
    "    max_output_value = 255   # 출력 픽셀 강도의 최대값\n",
    "    neighborhood_size = 99\n",
    "    subtract_from_mean = 15\n",
    "    image_binarized = cv2.adaptiveThreshold(image_enhanced,\n",
    "                                          255,\n",
    "                                          cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                          cv2.THRESH_BINARY,\n",
    "                                          99,\n",
    "                                          15)\n",
    "    \n",
    "    # bgr 변환\n",
    "    bgr_image = cv2.cvtColor(image_binarized,cv2.COLOR_GRAY2BGR)  \n",
    "    return bgr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noot-WroH5_u"
   },
   "outputs": [],
   "source": [
    "# morphology -> adaptive threshold\n",
    "def morph_threshold(imgpath):\n",
    "    # graysacle\n",
    "    img = load_img(imgpath)\n",
    "    img = img_to_array(img)\n",
    "    gray_image = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = gray_image.astype('uint8')\n",
    "    \n",
    "    # morph gradient\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    img2_grad = cv2.morphologyEx(gray_image, cv2.MORPH_GRADIENT, kernel)\n",
    "    \n",
    "    # adaptive threshold\n",
    "    th = cv2.adaptiveThreshold(img2_grad, \n",
    "                               255, \n",
    "                               cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                               cv2.THRESH_BINARY,\n",
    "                               99,\n",
    "                               15)\n",
    "    \n",
    "    # bgr 변환\n",
    "    bgr_image = cv2.cvtColor(th,cv2.COLOR_GRAY2BGR) # result함수 돌리기 위해 3차원으로 변환 \n",
    "    return bgr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_fjU1jMH8sr"
   },
   "outputs": [],
   "source": [
    "# morph_gradient -> 이미지 대비 향상 -> adaptive threshold\n",
    "def morph_threshold_sole(imgpath):\n",
    "    #grayscale\n",
    "    img = load_img(imgpath)\n",
    "    img = img_to_array(img)\n",
    "    gray_image = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    gray_image = gray_image.astype('uint8')\n",
    "    \n",
    "    # 이미지 대비를 향상\n",
    "    image_enhanced = cv2.equalizeHist(gray_image)\n",
    "    \n",
    "    # morph_gradient\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    img2_grad = cv2.morphologyEx(image_enhanced, cv2.MORPH_GRADIENT, kernel)\n",
    "    \n",
    "    # Adaptive Thresholding 적용 \n",
    "    image_binarized = cv2.adaptiveThreshold(img2_grad,\n",
    "                                          255,\n",
    "                                          cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                          cv2.THRESH_BINARY,\n",
    "                                          99,\n",
    "                                          15)\n",
    "    \n",
    "    # bgr 변환\n",
    "    bgr_image = cv2.cvtColor(image_binarized,cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    \n",
    "    return bgr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AFWl08RXFWb"
   },
   "outputs": [],
   "source": [
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9QvbvKY0bFpy"
   },
   "outputs": [],
   "source": [
    "#comparison_img_path = \"/content/Data/augmented_validation/15/15_0_1275.jpeg\"\n",
    "#label_validate = np.load(\"/content/drive/My Drive/label_validate.npy\")\n",
    "\n",
    "def model_sim_performance(vectorizing_model,comparison_img_path, validation_matrix, label_validate):\n",
    "  # making img_matrix ( by concatenate original img vector and validation_matrix)\n",
    "  comparison_img_category = int(comparison_img_path.split(\"/\")[-1].split(\" \")[0])  ## 이거는 img_path에서 카테고리 번호에 해당하는 부분이 어딘지를 찾아서 적절히 바꿔 주어야 함.\n",
    "  \n",
    "  datagen = ImageDataGenerator(rescale=1. /255)\n",
    "  function = original # 위의 전처리 함수들 중 본인이 맡은 함수로 선택.\n",
    "  img = function(comparison_img_path)\n",
    "  x = expand_dims(img,0)\n",
    "  generator = datagen.flow(x, y = None, batch_size = 1, shuffle=False, sample_weight=None)\n",
    "  img_vector = vectorizing_model.predict_generator(generator, steps=1)\n",
    "  \n",
    "  matrix = np.concatenate((img_vector,validation_matrix))\n",
    "\n",
    "  # calculate cosine_similarity\n",
    "  cosine_sim_vector = cosine_similarity(matrix)[0][1:] # 자기자신을 제외한 나머지 벡터들과의 코사인 유사도 계산.\n",
    "\n",
    "\n",
    "  # check model performance with similarity\n",
    "  top100_index = cosine_sim_vector.argsort()[-100:]\n",
    "  top100_categories = label_validate[top100_index]\n",
    "  performance = (np.sum(top100_categories == comparison_img_category))\n",
    "\n",
    "  return performance\n",
    "\n",
    "#model_performance = model_sim_performance(vectorizing_model, comparison_img_path, validation_matrix, label_validate)\n",
    "#print(model_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "colab_type": "code",
    "id": "-z0ppaDnSzEt",
    "outputId": "c5b35dad-e8a7-4b02-a056-ff0badaa41cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "comparison_img_path = \"/content/drive/Shared drives/선빵팀 :) 2/TuningStar_cate/1 (1).jpg\"\n",
    "validation_matrix = np.load(\"/content/drive/Shared drives/선빵팀 :) 2/validation_matrix/validation_matrix_morph_threshold_sole.npy\")\n",
    "label_validate = np.load(\"/content/drive/My Drive/label_validate.npy\")\n",
    "vectorizing_model = load_model(\"/content/drive/Shared drives/선빵팀 :) 2/Model/vectorizing_model_morph_threshold_sole.h5\")\n",
    "\n",
    "model_performance = model_sim_performance(vectorizing_model,comparison_img_path, validation_matrix, label_validate)\n",
    "print(model_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "  model_performance_list=[]\n",
    "  wheel_pathes=glob.glob(\"/content/drive/Shared drives/선빵팀 :) 2/TuningStar_cate/*.jpg\")\n",
    "  vectorizing_model = vectorizing_model\n",
    "  label_validate = np.load(\"/content/drive/My Drive/label_validate.npy\")\n",
    "  validation_matrix = validation_matrix\n",
    "  for comparison_img_path in tqdm(wheel_pathes):\n",
    "    model_performance = model_sim_performance(vectorizing_model,comparison_img_path, validation_matrix, label_validate)\n",
    "    model_performance_list.append(model_performance)\n",
    "\n",
    "  performance_array = np.array(model_performance_list)\n",
    "  print(np.mean(performance_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUDIh5lQJH1k"
   },
   "outputs": [],
   "source": [
    "# original : 44.98\n",
    "# Adaptive Threshold : 67.30\n",
    "# Morphology_equalizeHist_Adaptive : 48.75\n",
    "# equalizeHist_Adaptive Threshold : 48.20\n",
    "# Morphology_Adaptive Threshold : 41.03"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "similarity_perform_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ed1a26a7329474ea90d402432bda14e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22d50aa9eb004310b91f5ee07a86bc48",
      "max": 621,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4bdfa74a3f1a41589c870272e6cec3e1",
      "value": 621
     }
    },
    "22d50aa9eb004310b91f5ee07a86bc48": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33a359baa0414a7d9a7b1c886c6f7c21": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ed1a26a7329474ea90d402432bda14e",
       "IPY_MODEL_c2fe9d87de6746a0b879e861fb6b82ae"
      ],
      "layout": "IPY_MODEL_505a3af535dd4ac5917cdd1bb748dc7e"
     }
    },
    "4bdfa74a3f1a41589c870272e6cec3e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "505a3af535dd4ac5917cdd1bb748dc7e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f9df4ada0bb4662a7c4c9661dacf9a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2fe9d87de6746a0b879e861fb6b82ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8a5421ebc01482882f9e06ce2b17bc1",
      "placeholder": "​",
      "style": "IPY_MODEL_6f9df4ada0bb4662a7c4c9661dacf9a4",
      "value": " 621/621 [59:27&lt;00:00,  5.75s/it]"
     }
    },
    "e8a5421ebc01482882f9e06ce2b17bc1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
