{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNqEMD4GoDHo"
   },
   "source": [
    "# 1. training data와 validation data에 label 붙여주기\n",
    "학습을 시킬 때, ImageGenerator 옵션에서 'shuffle = False'인 경우 지정 경로에서 alphanumeric order로 데이터가 입력됩니다. 그래서 먼저 alphanumeric으로 숫자를 정렬해주고, label을 붙여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLuH6whk9QzC"
   },
   "outputs": [],
   "source": [
    "##alphanumeric order\n",
    "def sort(lst): \n",
    "    lst = [str(i) for i in lst] \n",
    "    lst.sort() \n",
    "    lst = [int(i) if i.isdigit() else i for i in lst ] \n",
    "    return lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bO41n7K_Hb3H"
   },
   "outputs": [],
   "source": [
    "## training data에 label 붙여주기\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "num_list = []\n",
    "for i in range(91):\n",
    "  num_list.append(i)\n",
    "\n",
    "## wheel_imgs_num dictionary : {카테고리 번호 : 카테고리 속하는 이미지 갯수}\n",
    "wheel_imgs_num = {}\n",
    "\n",
    "for idx in num_list:\n",
    "  wheel_img_path = glob.glob('/home/qwe3142/프로젝트/Data Augmentation/%s/*.jpeg' % idx) ## %s 부분은 '카테고리 번호명인 폴더'를 의미하니, 참고하여 경로를 설정해주시면 됩니다.  \n",
    "  wheel_imgs_num[idx] = len(wheel_img_path)\n",
    "\n",
    "## 빈 array를 만들고, alphanumeric order로 레이블 만들기 : wheel_imgs_num.keys()를 sort함수 적용하면 alphanumeric order로 순회합니다.\n",
    "## 빈 array에 np.concatenate로 numpy 배열 붙여나가는 방식으로 labeling 했습니다.\n",
    "label_trained = np.array([], dtype ='int32')\n",
    "for idx in sort(wheel_imgs_num.keys()):\n",
    "  x = np.array([idx]*wheel_imgs_num[idx], dtype ='int32') ## [카테고리 번호]를 카테고리 속하는 이미지 갯수만큼 배열 만들기\n",
    "  label_trained = np.concatenate((label_trained,x)) ## 배열 붙이기\n",
    "label_trained\n",
    "\n",
    "nb_train_samples = len(label_trained) ## 총 training data의 갯수는 최종 만들어진 training label의 갯수와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35099,
     "status": "ok",
     "timestamp": 1593043849453,
     "user": {
      "displayName": "‍박세웅[학생](경영대학 경영학과)",
      "photoUrl": "",
      "userId": "02497672298865538770"
     },
     "user_tz": -540
    },
    "id": "XXLuXn5RAKlX",
    "outputId": "6996e170-5b98-4bda-c83a-c5d800df1810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PH4rA9-zefM7"
   },
   "outputs": [],
   "source": [
    "## validation data에 label 붙여주기\n",
    "\n",
    "## wheel_imgs_num dictionary : {카테고리 번호 : 카테고리 속하는 이미지 갯수}\n",
    "\n",
    "wheel_imgs_num = {}\n",
    "\n",
    "## 위에서 만든 num_list\n",
    "for idx in num_list:\n",
    "  wheel_img_path = glob.glob('/home/qwe3142/프로젝트/augmented_validation/%s/*.jpeg' % idx) ## %s 부분은 '카테고리 번호 명'인 폴더를 의미하니, 참고하여 경로를 설정해주시면 됩니다. \n",
    "  wheel_imgs_num[idx] = len(wheel_img_path)\n",
    "\n",
    "## 빈 리스트를 만들고, alphanumeric order로 레이블 만들기 : wheel_imgs_num.keys()를 sort함수 적용하면 alphanumeric order로 순회합니다.\n",
    "## 빈 array에 np.concatenate로 numpy 배열 붙여나가는 방식으로 labeling 했습니다.\n",
    "label_validate = np.array([], dtype ='int32')\n",
    "for idx in sort(wheel_imgs_num.keys()):\n",
    "  x = np.array([idx]*wheel_imgs_num[idx], dtype ='int32') ## [카테고리 번호]를 카테고리 속하는 이미지 갯수만큼 배열 만들기\n",
    "  label_validate = np.concatenate((label_validate,x)) ## 배열 붙이기\n",
    "label_validate\n",
    "\n",
    "np.save('원하는 경로/label_validate.npy',label_validate) ## VM에서 학습하고 가져올 때, 불러서 len(label_validate) 입력할 수 있음.\n",
    "\n",
    "nb_validation_samples = len(label_validate) ## 총 validation data의 갯수는 최종 만들어진 validate label의 갯수와 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MJlRwKu_uvJ"
   },
   "source": [
    "#2. Transfer Learning 1 : ResNet50 계층 그대로 가져다 쓰기\n",
    "전이학습에는 계층을 그대로 가져다 쓰는 방법과, 계층 중에서 일부를 더 학습시키는 방법(fine-tuning)이 있습니다. 첫 번째 방법은 분류기를 제외한 사전 학습된 모델을 가져온 다음 그 위에 사용자에게 맞게 fully-connected 계층을 얹어서 모델을 구축합니다. 기존 예제코드에서 돌려볼 수 있는 부분은 돌려보면서 바로 사용할 수 있게끔 만들었습니다.\n",
    "\n",
    "참고 : https://keraskorea.github.io/posts/2018-10-24-little_data_powerful_model/ <- 케라스 공식 블로그, 작은 데이터셋으로 강력한 이미지 분류 모델 설계하기\n",
    "\n",
    "https://gist.github.com/fchollet/f35fbc80e066a49d65f1688a7e99f069 <- 참고한 원본 코드\n",
    "\n",
    "https://keras.io/ko/getting-started/functional-api-guide/ <- 케라스 api 한국어 버전\n",
    "\n",
    "##*6.24 추가내용\n",
    "\n",
    "1) Model 가중치 뿐 아니라 Model 전체(구조+가중치) 저장할 수 있는 메소드 추가(model.save)\n",
    "\n",
    "2) 커널 끊겼을 시 다시 불러오기 위해 epoch마다 저장할 수 있는 메소드 추가\n",
    "\n",
    "3) label_trained, label_validate 불러올 수 있는 코드 추가\n",
    "\n",
    "4) GPU 활용하고 싶으면 with tf.device(\"/device:GPU:0\"): 이 블럭 안에 코드를 넣어야 GPU 활용이 됩니다.\n",
    "\n",
    "5) model weight랑 model 전체를 저장하는 코드를 모두 반영하면서, 파일 이름이 바뀌었으니 꼭 확인 한 번 해주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 103023,
     "status": "ok",
     "timestamp": 1593028698310,
     "user": {
      "displayName": "‍박세웅[학생](경영대학 경영학과)",
      "photoUrl": "",
      "userId": "02497672298865538770"
     },
     "user_tz": -540
    },
    "id": "2Ph9fKOjk3fp",
    "outputId": "f4da3581-a6b9-4b45-b3b3-0f5127871820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 126399 samples, validate on 10101 samples\n",
      "Epoch 1/50\n",
      "126399/126399 [==============================] - 5s 42us/step - loss: 2.0277 - accuracy: 0.4544 - val_loss: 0.9146 - val_accuracy: 0.7547\n",
      "[]\n",
      "Epoch 2/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.9954 - accuracy: 0.7012 - val_loss: 0.5809 - val_accuracy: 0.8351\n",
      "[]\n",
      "Epoch 3/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.7561 - accuracy: 0.7668 - val_loss: 0.4442 - val_accuracy: 0.8675\n",
      "[]\n",
      "Epoch 4/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.6425 - accuracy: 0.7993 - val_loss: 0.3710 - val_accuracy: 0.8909\n",
      "[]\n",
      "Epoch 5/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.5706 - accuracy: 0.8203 - val_loss: 0.3094 - val_accuracy: 0.9081\n",
      "[]\n",
      "Epoch 6/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.5185 - accuracy: 0.8353 - val_loss: 0.2740 - val_accuracy: 0.9156\n",
      "[]\n",
      "Epoch 7/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.4819 - accuracy: 0.8462 - val_loss: 0.2653 - val_accuracy: 0.9135\n",
      "[]\n",
      "Epoch 8/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.4582 - accuracy: 0.8537 - val_loss: 0.2355 - val_accuracy: 0.9232\n",
      "[]\n",
      "Epoch 9/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.4349 - accuracy: 0.8609 - val_loss: 0.2291 - val_accuracy: 0.9284\n",
      "[]\n",
      "Epoch 10/50\n",
      "126399/126399 [==============================] - 5s 40us/step - loss: 0.4168 - accuracy: 0.8661 - val_loss: 0.2149 - val_accuracy: 0.9323\n",
      "[]\n",
      "Epoch 11/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.4052 - accuracy: 0.8699 - val_loss: 0.1976 - val_accuracy: 0.9362\n",
      "[]\n",
      "Epoch 12/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.3895 - accuracy: 0.8758 - val_loss: 0.1783 - val_accuracy: 0.9427\n",
      "[]\n",
      "Epoch 13/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.3824 - accuracy: 0.8772 - val_loss: 0.1809 - val_accuracy: 0.9399\n",
      "[]\n",
      "Epoch 14/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.3738 - accuracy: 0.8803 - val_loss: 0.1850 - val_accuracy: 0.9388\n",
      "[]\n",
      "Epoch 15/50\n",
      "126399/126399 [==============================] - 5s 39us/step - loss: 0.3645 - accuracy: 0.8835 - val_loss: 0.1876 - val_accuracy: 0.9356\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import applications\n",
    "from keras.callbacks import EarlyStopping, LambdaCallback, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    " \n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224 ##프로젝트 활용 이미지의 크기\n",
    "\n",
    "################필요한 경우에만 활용######################\n",
    "label_trained = np.load('/content/drive/My Drive/프로젝트/NewModel (1)/label_trained.npy')\n",
    "label_validate = np.load('/content/drive/My Drive/프로젝트/NewModel (1)/label_validate.npy')\n",
    "\n",
    "\n",
    "top_model_weights_path = '/content/drive/My Drive/프로젝트/bottleneck_fc_model_weight.h5' ## 뒤의 bottleneck_fc_model_weight.h5는 지우시면 안됩니다.\n",
    "top_model_path = '/content/drive/My Drive/프로젝트/bottleneck_fc_model_total.h5' ## 모델 전체를 저장할 경로\n",
    "#train_data_dir = '/home/qwe3142/프로젝트/Data Augmentation' #~~/training data/182(카테고리 숫자)/*.jpg 경로가 이렇게 생겼으면 traning data 까지만 복사해서 넣어주시면 됩니다.(맨 뒤에 '/'붙일 필요X)\n",
    "#validation_data_dir = '/home/qwe3142/프로젝트/augmented_validation'  #~~/validation data/182(카테고리 숫자)/*.jpg 경로가 이렇게 생겼으면 validation data 까지만 복사해서 넣어주시면 됩니다.(맨 뒤에 '/'붙일 필요X)\n",
    "nb_train_samples = len(label_trained) ## 총 training data의 갯수는 최종 만들어진 training label의 갯수와 같습니다.\n",
    "nb_validation_samples = len(label_validate) ## 총 validation data의 갯수는 최종 만들어진 validation label의 갯수와 같습니다.\n",
    "epochs = 50 ## 학습 에폭 수\n",
    "batch_size = 128 ## 학습 당 batch_size\n",
    "\n",
    "## bottleneck_features : classifier 직전의 vector를 의미합니다. (ResNet 50을 통과시킨) feature vector와 같은 의미입니다.\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the ResNet50 Network , imagenet의 가중치를 그대로 가져오고, include_top = False로 두어 classifier 부분은 부르지 않습니다.\n",
    "    model = applications.ResNet50(include_top=False, weights='imagenet')\n",
    "    # Shuffle을 false로 두어야 카테고리가 뒤섞이지 않고 순서대로 들어갑니다. 케라스 API에 따르면, alphanumeric 순서를 따른다고 합니다.\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator)\n",
    "    # np.save / np.load : 경로 지정한 곳으로 저장, 불러오기 가능, .npy확장자로 save하면 뽑힌 feature vector들이 저장됩니다.\n",
    "    np.save('/home/qwe3142/프로젝트/model_features/bottleneck_features_train.npy', \n",
    "            bottleneck_features_train) ## 뒤의 bottleneck_features_train.npy는 지우시면 안됩니다.\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator)\n",
    "    np.save('/home/qwe3142/프로젝트/model_features/bottleneck_features_validation.npy',\n",
    "            bottleneck_features_validation) ## 뒤의 bottleneck_features_validation.npy는 지우시면 안됩니다.\n",
    "\n",
    "    # 위에서 저장한 feature vector들을 불러와서, 순서대로 label을 붙여주고 우리 목적에 맞는 classfier를 만들어서 학습시킵니다. 이 때 label은 위에서 만든 label을 활용합니다.\n",
    "def train_top_model():\n",
    "    train_data = np.load('/content/drive/My Drive/프로젝트/NewModel (1)/bottleneck_features_train.npy') ##bottleneck_features_train.npy 저장 설정했던 경로\n",
    "    train_labels = label_trained\n",
    "\n",
    "    validation_data = np.load('/content/drive/My Drive/프로젝트/NewModel (1)/bottleneck_features_validation.npy') ##bottleneck_features_validation.npy 저장 설정했던 경로\n",
    "    validation_labels = label_validate\n",
    "    \n",
    "    # 새로 쌓은 모델의 계층. 예제 코드에서 가져왔습니다.\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(91, activation='softmax')) #dense는 분류기 갯수만큼\n",
    "\n",
    "    # 어떻게 학습시킬지 complie 메소드로 정해줄 수 있습니다.\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # print_weights와 early_stopping은 학습 과정에 추가할 수 있는 옵션입니다.\n",
    "    # early stopping은 학습에 진전이 없을 경우 학습을 종료하는 옵션이며, print_weights는(LambdaCallback) 학습이 잘 되고 있는지 특정 계층을 모니터링 할 수 있는 옵션입니다.\n",
    "    # 여기서는 2번째 layer를 기준으로 가중치가 업데이트 되는지 확인할 수 있습니다.\n",
    "    # check_point는 epoch마다 model을 저장합니다. model architecture와 weight 모두 저장합니다.\n",
    "    \n",
    "    print_weights = LambdaCallback(on_epoch_end=lambda epoch, logs: print(model.layers[1].get_weights()))\n",
    "    early_stopping = EarlyStopping(patience=3, mode='auto', monitor='val_loss')\n",
    "    check_point = ModelCheckpoint(filepath=top_model_path, monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "    \n",
    "\n",
    "\n",
    "    # fit 함수로 학습을 시작합니다.\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "              callbacks=[early_stopping, print_weights,check_point])\n",
    "    \n",
    "    #학습이 끝난 뒤, 모델을 저장합니다. 지정해 준 경로에 저장됩니다.\n",
    "    model.save_weights(top_model_weights_path) ##가중치\n",
    "    model.save(top_model_path) ##전체 모델(구조+가중치)\n",
    "\n",
    "#save_bottlebeck_features()\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "  train_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPiIzWXjARWu"
   },
   "source": [
    "#3. Transfer Learning 2 : ResNet50 계층 일부분을 추가로 학습시켜서 활용하기(Fine-tuning)\n",
    "기존에 학습되어있는 fully-connected 계층을 가져와서(위에서 학습시킨 모델 가중치) ResNet50 위에 얹습니다. 이후 ResNet50 계층 구조를 참고하여 학습시키지 않을 부분은 동결시키고(non-trainable) 학습시킬 부분만 살려서 학습을 진행합니다.(중간에 non-trainable 계층 갯수는 생각해봐야 할 부분) 참고 페이지에서도 말하듯 fine tuning의 핵심은 미세 조정이기 때문에 learning rate를 낮게 설정하고, 또한 사례에서는 모델의 마지막 계층만을 추가로 학습시켰습니다.\n",
    "\n",
    "참고 : https://keraskorea.github.io/posts/2018-10-24-little_data_powerful_model/ <- 케라스 공식 블로그, 작은 데이터셋으로 강력한 이미지 분류(Transfer Learning 1과 동일)\n",
    "\n",
    "https://gist.github.com/fchollet/7eb39b44eb9e16e59632d25fb3119975 <- 원본 코드\n",
    "\n",
    "https://eremo2002.tistory.com/76 <- ResNet50 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vDJgZH40CC9D"
   },
   "source": [
    "#### 6월 23일 Trnasfer_Learning 2 수정\n",
    "\n",
    "1. new_model 추가 : 기존의 model(ResNet50)에다 바로 top_train_model을 얹으려니 에러가 발생했습니다. model은 Sequential()이 있는 형태에서만 add가 가능하다고 하여, new_model을 만들고 거기에 ResNet50 모델과 top_train_model을 이어서 얹는 방식으로 구성했습니다.\n",
    "\n",
    "2. load_weights 에러 해결 : keras에 있는 버그라고 하는데, top_model의 첫 Dense 계층에 input_dim을 설정해줌으로써 해결했습니다.\n",
    "\n",
    "3. model.complie : ImageDataGenerator에서 class_mode=categorical로 주면, categorical_crossentropy로 loss함수를 지정해줘야 합니다.\n",
    "\n",
    "4. new_model : new_model_layers[0]까지 들어가야 ResNet을 열어볼 수 있어서 반영해주었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuWsm5LXGqpw"
   },
   "source": [
    "### 6월 24일 추가내용\n",
    "1. Colab에서 불러서 할 때, 해당 노트에서 압축을 풀고 그 상태로 학습을 시켜야합니다. Train data와 Validation data를 따로 압축해서 각각 다른 경로에 푸는 걸 추천합니다. 밑의 tarfile은 tar 확장자 파일 압축을 푸는 라이브러리입니다.\n",
    "\n",
    "2. GPU를 사용하고 싶은 경우, with tf.device(\"/device:GPU:0\"): 이 블럭 안에 코드를 넣어야 GPU 활용이 됩니다.\n",
    "\n",
    "3. Transfer Learning 1과 마찬가지로 model.save, callback.checkpoint 추가해주었습니다.\n",
    "\n",
    "4. label_trained, label_validate 불러올 수 있는 코드 추가\n",
    "\n",
    "5. ResNet50 얼리는 계층을 수정(conv5에서도 마지막 cnn block들만을 얼렸음)\n",
    "\n",
    "출처 : https://eremo2002.tistory.com/76 ResNet50 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KYZLeLuCC9D"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "  fname = '/content/drive/Shared drives/프로젝트 5_NewModel/NewModel/Data Augmentation2.tar'  # 압축 파일을 지정해주고   \n",
    "  ap = tarfile.open(fname)      # 열어줍니다. \n",
    "\n",
    "  ap.extractall('/content/drive/Shared drives/프로젝트 3_traindata')         # 그리고는 압축을 풀어줍니다. \n",
    "  # () 안에는 풀고 싶은 경로를 넣어주면 되요. 비워둘 경우 현재 경로에 압축 풉니다. \n",
    "\n",
    "  ap.close()  \n",
    "\n",
    "  fname = '/content/drive/Shared drives/프로젝트 5_NewModel/NewModel/augmented_validation2.tar'  # 압축 파일을 지정해주고   \n",
    "  ap = tarfile.open(fname)      # 열어줍니다. \n",
    "\n",
    "  ap.extractall('/content/drive/Shared drives/프로젝트 4_valdata')         # 그리고는 압축을 풀어줍니다. \n",
    "  # () 안에는 풀고 싶은 경로를 넣어주면 되요. 비워둘 경우 현재 경로에 압축 풉니다. \n",
    "\n",
    "  ap.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeZNDuS79aAq"
   },
   "outputs": [],
   "source": [
    "fname = '/content/drive/Shared drives/프로젝트 5_NewModel/NewModel/augmented_validation2.tar'  # 압축 파일을 지정해주고   \n",
    "ap = tarfile.open(fname)      # 열어줍니다. \n",
    "\n",
    "ap.extractall('/content/drive/Shared drives/프로젝트 6')         # 그리고는 압축을 풀어줍니다. \n",
    "# () 안에는 풀고 싶은 경로를 넣어주면 되요. 비워둘 경우 현재 경로에 압축 풉니다. \n",
    "\n",
    "ap.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qRy8hY4M86zm"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "model = load_model('/content/drive/My Drive/프로젝트/NewModel/fine_tuned_model_total.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1287,
     "status": "ok",
     "timestamp": 1593063109104,
     "user": {
      "displayName": "‍박세웅[학생](경영대학 경영학과)",
      "photoUrl": "",
      "userId": "02497672298865538770"
     },
     "user_tz": -540
    },
    "id": "ihuaxL6G9eDe",
    "outputId": "55ec64dd-3135-4645-e585-20252aef7e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, None, None, 2048)  23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (None, 91)                547931    \n",
      "=================================================================\n",
      "Total params: 24,135,643\n",
      "Trainable params: 5,013,595\n",
      "Non-trainable params: 19,122,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8924077,
     "status": "ok",
     "timestamp": 1593055745085,
     "user": {
      "displayName": "‍박세웅[학생](경영대학 경영학과)",
      "photoUrl": "",
      "userId": "02497672298865538770"
     },
     "user_tz": -540
    },
    "id": "_8t7tjpxAX-p",
    "outputId": "4f3468d7-9e1d-4620-b128-06ec9e9db1f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Found 126399 images belonging to 91 classes.\n",
      "Found 10101 images belonging to 91 classes.\n",
      "Epoch 1/50\n",
      "988/988 [==============================] - 705s 713ms/step - loss: 62.8207 - accuracy: 0.6578 - val_loss: 45.6045 - val_accuracy: 0.5688\n",
      "[array([[[[ 0.01781565,  0.02145215,  0.006597  , ..., -0.01568998,\n",
      "           0.01340635, -0.00725967],\n",
      "         [ 0.01948211, -0.01429684, -0.0179356 , ...,  0.01657623,\n",
      "           0.00880247,  0.01360396],\n",
      "         [ 0.0175682 ,  0.00889296, -0.00107875, ..., -0.0092871 ,\n",
      "           0.00964922, -0.00540653],\n",
      "         ...,\n",
      "         [ 0.02598654, -0.00650291, -0.02088963, ...,  0.02498965,\n",
      "          -0.00797103, -0.00211472],\n",
      "         [-0.01697466, -0.00676234, -0.00987928, ..., -0.00301424,\n",
      "           0.00972939, -0.01045766],\n",
      "         [-0.01674714, -0.00571006,  0.00526139, ...,  0.00505937,\n",
      "           0.00456144, -0.00247603]]]], dtype=float32), array([-1.9242294e-09, -2.0408464e-09,  3.0570633e-11, ...,\n",
      "        2.7004446e-10, -8.2112539e-10, -1.2703366e-09], dtype=float32)]\n",
      "Epoch 2/50\n",
      "988/988 [==============================] - 681s 689ms/step - loss: 1.8944 - accuracy: 0.9157 - val_loss: 0.1312 - val_accuracy: 0.9817\n",
      "[array([[[[ 0.01780426,  0.02145742,  0.00659682, ..., -0.01568849,\n",
      "           0.01339413, -0.00724521],\n",
      "         [ 0.01829214, -0.01329176, -0.01676038, ...,  0.01664695,\n",
      "           0.00896796,  0.01393679],\n",
      "         [ 0.01753188,  0.00896692, -0.00104879, ..., -0.00940711,\n",
      "           0.00951407, -0.00475114],\n",
      "         ...,\n",
      "         [ 0.0295032 , -0.00686154, -0.02076714, ...,  0.02567435,\n",
      "          -0.00777122, -0.00308003],\n",
      "         [-0.01777329, -0.00660339, -0.00990848, ..., -0.00300717,\n",
      "           0.00964774, -0.01033742],\n",
      "         [-0.01677814, -0.00574113,  0.00524506, ...,  0.00507254,\n",
      "           0.00456695, -0.00251125]]]], dtype=float32), array([-1.7458479e-09, -1.9277846e-09,  1.4763993e-10, ...,\n",
      "        4.4540906e-11, -8.8393715e-10, -7.1106532e-10], dtype=float32)]\n",
      "Epoch 3/50\n",
      "988/988 [==============================] - 681s 689ms/step - loss: 0.8049 - accuracy: 0.9493 - val_loss: 0.1281 - val_accuracy: 0.9869\n",
      "[array([[[[ 0.01777733,  0.02146124,  0.00659325, ..., -0.01568804,\n",
      "           0.0133893 , -0.00724631],\n",
      "         [ 0.0178996 , -0.01360707, -0.0166016 , ...,  0.01656591,\n",
      "           0.00822873,  0.01331319],\n",
      "         [ 0.01761649,  0.0090372 , -0.00103106, ..., -0.00947816,\n",
      "           0.0094823 , -0.00445119],\n",
      "         ...,\n",
      "         [ 0.0262489 , -0.0064602 , -0.02091971, ...,  0.02593634,\n",
      "          -0.00713548, -0.00317937],\n",
      "         [-0.01762963, -0.00648628, -0.00989765, ..., -0.00304085,\n",
      "           0.00960222, -0.0102749 ],\n",
      "         [-0.01677343, -0.00577748,  0.00518352, ...,  0.00507921,\n",
      "           0.00459569, -0.00254726]]]], dtype=float32), array([-3.6516765e-09, -1.5256710e-09,  2.4085375e-10, ...,\n",
      "       -1.2663889e-10, -9.6393715e-10, -8.7168345e-10], dtype=float32)]\n",
      "Epoch 4/50\n",
      "988/988 [==============================] - 683s 691ms/step - loss: 0.4854 - accuracy: 0.9633 - val_loss: 0.0561 - val_accuracy: 0.9898\n",
      "[array([[[[ 0.01778871,  0.02146362,  0.00659137, ..., -0.01568665,\n",
      "           0.01338936, -0.00724259],\n",
      "         [ 0.01724675, -0.0138464 , -0.01673186, ...,  0.01716278,\n",
      "           0.008628  ,  0.01344816],\n",
      "         [ 0.01789771,  0.00905901, -0.00098357, ..., -0.00952655,\n",
      "           0.00945615, -0.00428751],\n",
      "         ...,\n",
      "         [ 0.02599746, -0.00648889, -0.02088937, ...,  0.02610024,\n",
      "          -0.00711793, -0.00360153],\n",
      "         [-0.01802281, -0.00646391, -0.00986585, ..., -0.00300402,\n",
      "           0.00963032, -0.01025697],\n",
      "         [-0.01673892, -0.00577296,  0.00521376, ...,  0.00508169,\n",
      "           0.00459607, -0.00255881]]]], dtype=float32), array([-4.4358064e-09, -1.1927550e-09,  2.7966721e-10, ...,\n",
      "       -1.7735728e-10, -1.0785965e-09, -4.9829407e-10], dtype=float32)]\n",
      "Epoch 5/50\n",
      "988/988 [==============================] - 682s 690ms/step - loss: 0.3367 - accuracy: 0.9715 - val_loss: 0.0035 - val_accuracy: 0.9921\n",
      "[array([[[[ 0.01777632,  0.0214646 ,  0.00658823, ..., -0.01568702,\n",
      "           0.01338673, -0.00723776],\n",
      "         [ 0.01720386, -0.01328602, -0.01640986, ...,  0.01767077,\n",
      "           0.00901007,  0.01351099],\n",
      "         [ 0.01791182,  0.00909173, -0.00094488, ..., -0.00954807,\n",
      "           0.00943727, -0.00414073],\n",
      "         ...,\n",
      "         [ 0.02835441, -0.00631954, -0.02094929, ...,  0.02599655,\n",
      "          -0.00669874, -0.00352768],\n",
      "         [-0.01796244, -0.00638133, -0.00986265, ..., -0.00302068,\n",
      "           0.00958288, -0.01022002],\n",
      "         [-0.01670655, -0.00577275,  0.00524834, ...,  0.00507796,\n",
      "           0.0045891 , -0.00257329]]]], dtype=float32), array([-3.9304213e-09, -1.0649157e-09,  4.4296788e-10, ...,\n",
      "       -2.0560680e-10, -1.0029912e-09, -8.5922332e-11], dtype=float32)]\n",
      "Epoch 6/50\n",
      "988/988 [==============================] - 681s 689ms/step - loss: 0.2559 - accuracy: 0.9764 - val_loss: 9.9637e-05 - val_accuracy: 0.9934\n",
      "[array([[[[ 0.01778246,  0.02146407,  0.00658837, ..., -0.01568662,\n",
      "           0.01338986, -0.00723673],\n",
      "         [ 0.01733424, -0.01376142, -0.01645412, ...,  0.01796279,\n",
      "           0.00895443,  0.01282765],\n",
      "         [ 0.01793906,  0.00909436, -0.00097892, ..., -0.0095717 ,\n",
      "           0.00942982, -0.00400455],\n",
      "         ...,\n",
      "         [ 0.02803656, -0.00616778, -0.02090969, ...,  0.02617333,\n",
      "          -0.00722907, -0.00371113],\n",
      "         [-0.01791087, -0.00637441, -0.00987114, ..., -0.00300652,\n",
      "           0.00960703, -0.01022306],\n",
      "         [-0.01673174, -0.00577227,  0.00523969, ...,  0.00508309,\n",
      "           0.00458431, -0.00256835]]]], dtype=float32), array([-3.0921057e-09, -9.6709796e-10,  3.8807019e-10, ...,\n",
      "       -2.0374921e-10, -9.9001396e-10, -1.7397724e-10], dtype=float32)]\n",
      "Epoch 7/50\n",
      "988/988 [==============================] - 681s 689ms/step - loss: 0.2048 - accuracy: 0.9804 - val_loss: 5.7057e-08 - val_accuracy: 0.9936\n",
      "[array([[[[ 0.01778678,  0.02146337,  0.00658554, ..., -0.01568691,\n",
      "           0.01338967, -0.00723536],\n",
      "         [ 0.01718458, -0.01329367, -0.01630141, ...,  0.01816938,\n",
      "           0.0089611 ,  0.01296849],\n",
      "         [ 0.01795023,  0.00908721, -0.00105877, ..., -0.00959388,\n",
      "           0.00943506, -0.00389968],\n",
      "         ...,\n",
      "         [ 0.02906996, -0.00622416, -0.02100362, ...,  0.02639589,\n",
      "          -0.00737628, -0.00343703],\n",
      "         [-0.01799165, -0.0063769 , -0.00983944, ..., -0.00299528,\n",
      "           0.00959121, -0.01018859],\n",
      "         [-0.01673225, -0.00577871,  0.0052382 , ...,  0.00508935,\n",
      "           0.0045839 , -0.00257344]]]], dtype=float32), array([-3.99073308e-09, -8.13128564e-10,  5.60153202e-10, ...,\n",
      "       -2.88163771e-10, -9.75860504e-10, -1.08270545e-10], dtype=float32)]\n",
      "Epoch 8/50\n",
      "988/988 [==============================] - 698s 707ms/step - loss: 0.1732 - accuracy: 0.9826 - val_loss: 2.0907e-04 - val_accuracy: 0.9956\n",
      "[array([[[[ 0.01778615,  0.02146305,  0.00658428, ..., -0.0156872 ,\n",
      "           0.01338734, -0.00723551],\n",
      "         [ 0.01875858, -0.0134882 , -0.01620575, ...,  0.01839612,\n",
      "           0.0088856 ,  0.01327852],\n",
      "         [ 0.01791883,  0.00909838, -0.0010541 , ..., -0.00961238,\n",
      "           0.00940508, -0.00382077],\n",
      "         ...,\n",
      "         [ 0.03000422, -0.00605151, -0.02097044, ...,  0.0265323 ,\n",
      "          -0.00732291, -0.00333593],\n",
      "         [-0.01803018, -0.00635842, -0.00982205, ..., -0.00300503,\n",
      "           0.00959367, -0.01017214],\n",
      "         [-0.01672113, -0.00577515,  0.00527543, ...,  0.0050894 ,\n",
      "           0.00457731, -0.00256746]]]], dtype=float32), array([-3.9618016e-09, -7.9494544e-10,  4.8351634e-10, ...,\n",
      "       -2.5187397e-10, -9.7791808e-10,  8.0693938e-11], dtype=float32)]\n",
      "Epoch 9/50\n",
      "988/988 [==============================] - 682s 690ms/step - loss: 0.1405 - accuracy: 0.9852 - val_loss: 0.2988 - val_accuracy: 0.9958\n",
      "[array([[[[ 0.01777165,  0.0214624 ,  0.00658382, ..., -0.0156872 ,\n",
      "           0.013387  , -0.00723315],\n",
      "         [ 0.01891058, -0.01326933, -0.01640102, ...,  0.01863241,\n",
      "           0.0086301 ,  0.01352197],\n",
      "         [ 0.01795127,  0.00910573, -0.00105602, ..., -0.00962267,\n",
      "           0.0094111 , -0.00378485],\n",
      "         ...,\n",
      "         [ 0.02810566, -0.00590136, -0.02095946, ...,  0.0266054 ,\n",
      "          -0.00747986, -0.00324608],\n",
      "         [-0.01804866, -0.00636388, -0.00983092, ..., -0.00300616,\n",
      "           0.00962504, -0.01017546],\n",
      "         [-0.01671454, -0.00577045,  0.00530661, ...,  0.00508364,\n",
      "           0.00458835, -0.00257152]]]], dtype=float32), array([-3.9299377e-09, -7.2006801e-10,  5.4775201e-10, ...,\n",
      "       -2.5767738e-10, -9.1921271e-10,  2.9107880e-10], dtype=float32)]\n",
      "Epoch 10/50\n",
      "988/988 [==============================] - 682s 690ms/step - loss: 0.1307 - accuracy: 0.9866 - val_loss: 6.1133e-09 - val_accuracy: 0.9964\n",
      "[array([[[[ 0.01776415,  0.02146434,  0.00658694, ..., -0.01568654,\n",
      "           0.01339122, -0.00723478],\n",
      "         [ 0.01847125, -0.01346486, -0.01648399, ...,  0.01881659,\n",
      "           0.0089483 ,  0.01355212],\n",
      "         [ 0.01803636,  0.00913087, -0.0010282 , ..., -0.00964181,\n",
      "           0.00938839, -0.00374689],\n",
      "         ...,\n",
      "         [ 0.02887362, -0.00571222, -0.02090693, ...,  0.0266323 ,\n",
      "          -0.00721819, -0.00312697],\n",
      "         [-0.01800235, -0.00635423, -0.00982008, ..., -0.00301599,\n",
      "           0.00961704, -0.01016099],\n",
      "         [-0.01668726, -0.00575748,  0.00531666, ...,  0.00507918,\n",
      "           0.00458958, -0.00254725]]]], dtype=float32), array([-3.8108570e-09, -5.5332011e-10,  6.1788763e-10, ...,\n",
      "       -2.9486480e-10, -8.3290591e-10,  4.0971260e-10], dtype=float32)]\n",
      "Epoch 11/50\n",
      "988/988 [==============================] - 682s 690ms/step - loss: 0.1085 - accuracy: 0.9883 - val_loss: 3.4642e-07 - val_accuracy: 0.9959\n",
      "[array([[[[ 0.0177621 ,  0.02146406,  0.00657926, ..., -0.01568675,\n",
      "           0.01338803, -0.00723623],\n",
      "         [ 0.01968762, -0.01339942, -0.01643123, ...,  0.01897916,\n",
      "           0.00890627,  0.01361001],\n",
      "         [ 0.0179581 ,  0.00913969, -0.00108359, ..., -0.00965678,\n",
      "           0.00936052, -0.00369365],\n",
      "         ...,\n",
      "         [ 0.02857197, -0.00571878, -0.02097372, ...,  0.02647579,\n",
      "          -0.00705019, -0.00303925],\n",
      "         [-0.01801487, -0.00634246, -0.00984005, ..., -0.0030177 ,\n",
      "           0.00962805, -0.0101462 ],\n",
      "         [-0.01669367, -0.00575583,  0.00531   , ...,  0.00508136,\n",
      "           0.0045822 , -0.0025723 ]]]], dtype=float32), array([-4.2281063e-09, -5.3712262e-10,  5.3771326e-10, ...,\n",
      "       -4.2240064e-10, -7.8340784e-10,  4.6406615e-10], dtype=float32)]\n",
      "Epoch 12/50\n",
      "988/988 [==============================] - 681s 690ms/step - loss: 0.0886 - accuracy: 0.9895 - val_loss: 0.6617 - val_accuracy: 0.9956\n",
      "[array([[[[ 0.01776662,  0.02146319,  0.00657987, ..., -0.01568701,\n",
      "           0.0133883 , -0.00723515],\n",
      "         [ 0.01974619, -0.0133047 , -0.01651791, ...,  0.01903184,\n",
      "           0.00851478,  0.01378263],\n",
      "         [ 0.01798075,  0.00914684, -0.00108045, ..., -0.00966268,\n",
      "           0.00935481, -0.0036369 ],\n",
      "         ...,\n",
      "         [ 0.02828356, -0.00586627, -0.02101061, ...,  0.02653927,\n",
      "          -0.00679444, -0.00312752],\n",
      "         [-0.01804525, -0.0063374 , -0.00982968, ..., -0.00301561,\n",
      "           0.0096498 , -0.01014859],\n",
      "         [-0.01669429, -0.00575819,  0.00532451, ...,  0.00508266,\n",
      "           0.00458167, -0.00256049]]]], dtype=float32), array([-3.8788208e-09, -4.1340822e-10,  6.3404076e-10, ...,\n",
      "       -3.7642259e-10, -8.0099638e-10,  6.9105438e-10], dtype=float32)]\n",
      "Epoch 13/50\n",
      "988/988 [==============================] - 683s 691ms/step - loss: 0.0858 - accuracy: 0.9902 - val_loss: 5.6500e-05 - val_accuracy: 0.9962\n",
      "[array([[[[ 0.01776453,  0.02146315,  0.00657633, ..., -0.01568692,\n",
      "           0.01338842, -0.00723429],\n",
      "         [ 0.02036465, -0.01335899, -0.01655966, ...,  0.01913745,\n",
      "           0.00853212,  0.01356859],\n",
      "         [ 0.01804698,  0.00916345, -0.00112607, ..., -0.00967331,\n",
      "           0.00935828, -0.00358544],\n",
      "         ...,\n",
      "         [ 0.02773909, -0.0057533 , -0.02104725, ...,  0.02636266,\n",
      "          -0.00701728, -0.00356431],\n",
      "         [-0.01806678, -0.00633177, -0.0098245 , ..., -0.00301742,\n",
      "           0.00963888, -0.01013873],\n",
      "         [-0.01669586, -0.005764  ,  0.00533842, ...,  0.00507815,\n",
      "           0.00458123, -0.00255624]]]], dtype=float32), array([-3.9795616e-09, -4.1267625e-10,  4.9919668e-10, ...,\n",
      "       -4.2683837e-10, -8.8090962e-10,  7.0299877e-10], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.callbacks import EarlyStopping, LambdaCallback, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "label_trained = np.load('/content/drive/Shared drives/프로젝트 5_NewModel/NewModel/label_trained.npy')\n",
    "label_validate = np.load('/content/drive/Shared drives/프로젝트 5_NewModel/NewModel/label_validate.npy')\n",
    "\n",
    "# path to the model weights files.\n",
    "## top_model_weights : 얹기 전에 미리 학습을 시켜야 하는데, 위에서 학습한 모델 가중치 불러와서 활용하면 됩니다.\n",
    "top_model_weights_path = '/content/drive/My Drive/프로젝트/NewModel/bottleneck_fc_model_weight.h5' ## 먼저 학습이 된 top model의 weight를 불러와야 합니다. 위의 top_model_weights_path를 가져오시면 됩니다.\n",
    "\n",
    "## fine tuned 모델 저장\n",
    "fine_tuned_model_weight_path = '/content/drive/My Drive/프로젝트/NewModel/fine_tuned_model_weight.h5'\n",
    "fine_tuned_model_total_path = '/content/drive/My Drive/프로젝트/NewModel/fine_tuned_model_total.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "train_data_dir = '/content/drive/Shared drives/프로젝트 3_traindata' # ~~/training data/182(카테고리 숫자)/*.jpg 경로가 이렇게 생겼으면 traning data 까지만 복사해서 넣어주시면 됩니다.(뒤에 '/'붙일 필요 없음)\n",
    "validation_data_dir = '/content/drive/Shared drives/프로젝트 4_valdata' # ~~/validation data/182(카테고리 숫자)/*.jpg 경로가 이렇게 생겼으면 validation data 까지만 복사해서 넣어주시면 됩니다.(뒤에 '/'붙일 필요 없음)\n",
    "nb_train_samples = len(label_trained) ## 총 training data의 갯수는 최종 만들어진 training label의 갯수와 같습니다.\n",
    "nb_validation_samples = len(label_validate) ## 총 validation data의 갯수는 최종 만들어진 validation label의 갯수와 같습니다.\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "# build the ResNet50 network(사용자에 맞게)\n",
    "model = applications.ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "print('Model loaded.')\n",
    "\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "#top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu', input_dim = 2048))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(91, activation='softmax'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(model)\n",
    "new_model.add(GlobalAveragePooling2D())\n",
    "new_model.add(top_model)\n",
    "\n",
    "\n",
    "## ResNet50 기준으로 conv5(마지막 convolution layer)만 살리고 나머지는 동결,\n",
    "## conv 5의 가장 끝단 CNN만 학습시키고 나머지는 다 얼렸습니다.\n",
    "for layer in new_model.layers[0].layers[:155]:\n",
    "    layer.trainable=False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "new_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generator로 데이터 준비합니다. 이 때는 상위 폴더의 이름(카테고리 명)이 바로 label로 붙도록, class_mode를 categorical로 지정해줍니다.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# print_weights와 early_stopping은 학습 과정에 추가할 수 있는 옵션입니다.\n",
    "# early stopping은 학습에 진전이 없을 경우 학습을 종료하는 옵션이며, print_weights는(LambdaCallback) 학습이 잘 되고 있는지 특정 계층을 모니터링 할 수 있는 옵션입니다.\n",
    "# 여기서는 171번째 layer를 기준으로 가중치가 업데이트 되는지 확인할 수 있습니다.(convolution 2D 계층, ResNet50의 마지막 Convolution 계층) \n",
    "print_weights = LambdaCallback(on_epoch_end=lambda epoch, logs: print(new_model.layers[0].layers[171].get_weights()))\n",
    "early_stopping = EarlyStopping(patience=3, mode='auto', monitor='val_loss')\n",
    "check_point = ModelCheckpoint(filepath=fine_tuned_model_total_path, monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "\n",
    "# fine-tune the model, 여기서는 generator로 데이터가 들어오므로 fit_generator를 활용해줍니다.\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "  new_model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping, print_weights, check_point])\n",
    "\n",
    "  # 학습이 끝난 뒤 모델을 저장합니다. 위와 마찬가지로 경로만 바꿔주고 뒤에 h5 파일은 그대로 두셔야 합니다.\n",
    "  new_model.save_weights(fine_tuned_model_weight_path)\n",
    "  new_model.save(fine_tuned_model_total_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transfer_Learning_최종(0624).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
